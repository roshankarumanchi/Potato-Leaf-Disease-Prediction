##Import Required Modules
import tensorflow as tf 
from tensorflow.Keras import models, layers 
import matplotlib.pyplot as plt 
from google.Ecolab import drive 
drive.mount('/content/drive') 
Mounted at /content/drive 
##Import data set
BATCH_SIZE = 32
IMAGE_SIZE = 256
CHANNELS=3
EPOCHS=10
dataset = tf.keras.preprocessing.image_dataset_from_directory( 
 "/content/drive/MyDrive/potato", 
 seed=123, 
 shuffle=True, 
 image_size=(IMAGE_SIZE,IMAGE_SIZE), 
 batch_size=BATCH_SIZE 
) 
Found 2152 files belonging to 1 class. 
class_names = dataset.class_names 
class_names 
['PlantVillage'] 
for image_batch, labels_batch in dataset.take(1): 
 print(image_batch.shape) 
 print(labels_batch.numpy()) 
(32, 256, 256, 3) 
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] 
Len(dataset) 
##Visualize some of the images from our dataset
for image_batch, and labels_batch in the dataset.take(1): 
 plt.imshow(image_batch[0].numpy().astype("uint8")) 
 22 
plt.figure(figsize=(10, 10)) 
for image_batch, labels_batch in dataset.take(1): 
 for i in range(12): 
 ax = plt.subplot(3, 4, i + 1) 
 plt.imshow(image_batch[i].numpy().astype("uint8")) 
 plt.title(class_names[labels_batch[i]]) 
 plt.axis("off") 
##Function to Split Dataset
##Dataset should be bifurcated into 3 subsets, namely:
##Training: Dataset to be used while training
##Validation: Dataset to be tested against while training
##Test: Dataset to be tested against after we trained a model
Len(dataset) 
68 
train_size = 0.8
Len(dataset)*train_size 
54.400000000000006 
train_ds = dataset.take(54) 
Len(train_ds) 
14 
val_size=0.1
Len(dataset)*val_size 
6.800000000000001 
test_ds = test_ds.skip(6) 
Len(test_ds) 
8 
def get_dataset_partitions_tf(ds, train_split=0.8, val_split=0.1, test_split=0.1, 
shuffle=True, shuffle_size=10000): 
 assert (train_split + test_split + val_split) == 1
ds_size = len(ds) 
 if shuffle: 
 ds = ds.shuffle(shuffle_size, seed=12) 
 train_size = int(train_split * ds_size) 
 val_size = int(val_split * ds_size) 
 train_ds = ds.take(train_size) 
 val_ds = ds.skip(train_size).take(val_size) 
 test_ds = ds.skip(train_size).skip(val_size) 
 return train_ds, val_ds, test_ds 
train_ds, val_ds, test_ds = get_dataset_partitions_tf(dataset) 
Len(train_ds) 
54 
Len(val_ds) 
6 
##Cache, Shuffle, and Prefetch the Dataset
train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE) 
val_ds = val_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE) 
test_ds = test_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE) 
##Building the Model
##Creating a Layer for Resizing and Normalization
resize_and_rescale = tf. keras.Sequential([ 
 layers.experimental.preprocessing.Resizing(IMAGE_SIZE, IMAGE_SIZE), 
 layers.experimental.preprocessing.Rescaling(1./255), 
]) 
##Data Augmentation
data_augmentation = tf. keras.Sequential([ 
 layers.experimental.preprocessing.RandomFlip("horizontal_and_vertical"), 
 layers.experimental.preprocessing.RandomRotation(0.2), 
]) 
##Applying Data Augmentation to Train Dataset
train_ds = train_ds.map( 
 lambda x, y: (data_augmentation(x, training=True), y) 
).prefetch(buffer_size=tf.data.AUTOTUNE) 
##Model Architecture
input_shape = (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, CHANNELS) 
n_classes = 3
model = models.Sequential([ 
 resize_and_rescale, 
 layers.Conv2D(32, kernel_size = (3,3), activation='relu', 
input_shape=input_shape), 
 layers.MaxPooling2D((2, 2)), 
 layers.Conv2D(64, kernel_size = (3,3), activation='relu'), 
 layers.MaxPooling2D((2, 2)), 
 layers.Conv2D(64, kernel_size = (3,3), activation='relu'),
layers.MaxPooling2D((2, 2)), 
 layers.Conv2D(64, (3, 3), activation='relu'), 
 layers.MaxPooling2D((2, 2)), 
 layers.Conv2D(64, (3, 3), activation='relu'), 
 layers.MaxPooling2D((2, 2)), 
 layers.Conv2D(64, (3, 3), activation='relu'), 
 layers.MaxPooling2D((2, 2)), 
 layers.Flatten(), 
 layers.Dense(64, activation='relu'), 
 layers.Dense(n_classes, activation='softmax'), 
]) 
model.build(input_shape=input_shape) 
model.summary() 
##Compiling the Model
model. compile( 
 optimizer='adam', 
 loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), 
 metrics=['accuracy'] 
) 
history = model.fit( 
 train_ds, 
 batch_size=BATCH_SIZE, 
 validation_data=val_ds, 
 verbose=1, 
 epochs=10, 
) 
#Plotting the Accuracy and Loss Curves
history 
<keras.callbacks.History at 0x1e821797d30> 
history. params 
{'verbose': 1, 'epochs': 10, 'steps': 54} 
 27 
history. history.keys() 
dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy']) 
type(history.history['loss']) 
Len(history.history['loss']) 
10 
history.history['loss'][:5] # show loss for first 5 epochs
[0.8121130466461182, 0.5993865728378296, 0.4441331624984741, 0.3546609580516815, 
0.2834084928035736] 
acc = history.history['accuracy'] 
val_acc = history.history['val_accuracy'] 
loss = history.history['loss'] 
val_loss = history.history['val_loss'] 
plt.figure(figsize=(8, 8)) 
plt.subplot(1, 2, 1) 
plt.plot(range(EPOCHS), acc, label='Training Accuracy') 
plt.plot(range(EPOCHS), val_acc, label='Validation Accuracy') 
plt.legend(loc='lower right') 
plt.title('Training and Validation Accuracy') 
plt.subplot(1, 2, 2) 
plt.plot(range(EPOCHS), loss, label='Training Loss') 
plt.plot(range(EPOCHS), val_loss, label='Validation Loss') 
plt.legend(loc='upper right') 
plt.title('Training and Validation Loss') 
plt.show() 
##Run prediction on a sample image 
import numpy as np 
for images_batch, labels_batch in test_ds.take(1): 
 first_image = images_batch[0].numpy().astype('uint8') 
 first_label = labels_batch[0].numpy() 
 print("first image to predict") 
 plt.imshow(first_image) 
 print("actual label:",class_names[first_label]) 
 batch_prediction = model.predict(images_batch) 
 print("predicted label:",class_names[np.argmax(batch_prediction[0])]) 
#Write a function for inference
def predict(model, img): 
 img_array = tf.keras.preprocessing.image.img_to_array(images[i].numpy()) 
 img_array = tf.expand_dims(img_array, 0) 
 predictions = model.predict(img_array) 
 predicted_class = class_names[np.argmax(predictions[0])] 
 confidence = round(100 * (np.max(predictions[0])), 2) 
 return predicted_class, confidence 
##Now run inference on few sample images
plt.figure(figsize=(15, 15)) 
for images, labels in test_ds.take(1): 
 for i in range(9): 
 ax = plt.subplot(3, 3, i + 1) 
 plt.imshow(images[i].numpy().astype("uint8")) 
 predicted_class, confidence = predict(model, images[i].numpy()) 
 actual_class = class_names[labels[i]] 
 plt.title(f"Actual: {actual_class},\n Predicted: {predicted_class}.\n 
Confidence: {confidence}%") 
 plt.axis("off") 
